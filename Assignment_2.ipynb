{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37584667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ### Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\n",
    "# In machine learning, kernel functions enable algorithms to operate in a high-dimensional space without explicitly computing the coordinates of the data in that space. This technique is known as the \"kernel trick.\" Polynomial kernels are a specific type of kernel function that allows Support Vector Machines (SVMs) to create decision boundaries that are polynomial curves rather than linear lines.\n",
    "\n",
    "# A polynomial kernel of degree \\( d \\) is defined as:\n",
    "# \\[ K(x, x') = (x \\cdot x' + c)^d \\]\n",
    "\n",
    "# where:\n",
    "# - \\( x \\) and \\( x' \\) are input vectors.\n",
    "# - \\( c \\) is a constant that can be adjusted.\n",
    "# - \\( d \\) is the degree of the polynomial.\n",
    "\n",
    "# ### Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "# You can implement an SVM with a polynomial kernel in Python using the Scikit-learn library by specifying the `kernel='poly'` parameter in the `SVC` class. Here is an example:\n",
    "\n",
    "# ```python\n",
    "# from sklearn import datasets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load the Iris dataset\n",
    "# iris = datasets.load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Create an SVM classifier with a polynomial kernel\n",
    "# svc = SVC(kernel='poly', degree=3, C=1, random_state=42)\n",
    "# svc.fit(X_train, y_train)\n",
    "\n",
    "# # Predict the labels for the testing set\n",
    "# y_pred = svc.predict(X_test)\n",
    "\n",
    "# # Compute the accuracy of the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "# ```\n",
    "\n",
    "# ### Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "# In Support Vector Regression (SVR), epsilon (\\(\\epsilon\\)) defines a margin of tolerance where no penalty is given to errors. Increasing the value of \\(\\epsilon\\) allows a larger margin of tolerance, which means more data points can fall within the margin without affecting the model. As a result, increasing \\(\\epsilon\\) typically reduces the number of support vectors because fewer data points lie outside the margin and thus contribute to the model.\n",
    "\n",
    "# ### Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)?\n",
    "\n",
    "# - **Kernel Function**: The kernel function determines the shape of the decision boundary. Common choices include linear, polynomial, and RBF (radial basis function) kernels. The choice of kernel affects the model's ability to capture complex relationships in the data.\n",
    "\n",
    "# - **C Parameter**: The regularization parameter \\( C \\) controls the trade-off between achieving a low error on the training data and minimizing the model complexity. A high \\( C \\) value aims for a low training error, potentially leading to overfitting, while a low \\( C \\) value allows more slack, which can lead to underfitting.\n",
    "\n",
    "# - **Epsilon Parameter**: The \\(\\epsilon\\) parameter defines the margin of tolerance where no penalty is given to errors. A larger \\(\\epsilon\\) creates a wider margin, reducing the number of support vectors and making the model less sensitive to small variations in the data.\n",
    "\n",
    "# - **Gamma Parameter**: The gamma parameter (used in RBF and polynomial kernels) defines the influence of a single training example. A low gamma value means a large radius of influence for each point, leading to a smoother decision boundary. A high gamma value means a small radius of influence, leading to a more complex decision boundary that can overfit the training data.\n",
    "\n",
    "# ### Q5. Assignment Implementation\n",
    "\n",
    "# Here is a full implementation of an SVM classifier using Scikit-learn, with hyperparameter tuning using GridSearchCV:\n",
    "\n",
    "# ```python\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn import datasets\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "# import joblib\n",
    "\n",
    "# # Load the dataset\n",
    "# iris = datasets.load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Create an instance of the SVC classifier\n",
    "# svc = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# # Define the parameter grid for GridSearchCV\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1, 10, 100],\n",
    "#     'gamma': [1, 0.1, 0.01, 0.001]\n",
    "# }\n",
    "\n",
    "# # Perform GridSearchCV to tune hyperparameters\n",
    "# grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and best score\n",
    "# print(f'Best parameters: {grid_search.best_params_}')\n",
    "# print(f'Best score: {grid_search.best_score_}')\n",
    "\n",
    "# # Train the tuned classifier on the entire dataset\n",
    "# best_svc = grid_search.best_estimator_\n",
    "# best_svc.fit(X_train, y_train)\n",
    "\n",
    "# # Save the trained classifier to a file\n",
    "# joblib.dump(best_svc, 'svm_classifier.pkl')\n",
    "\n",
    "# # Predict the labels for the testing set\n",
    "# y_pred = best_svc.predict(X_test)\n",
    "\n",
    "# # Evaluate the performance of the classifier\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred, average='weighted')\n",
    "# recall = recall_score(y_test, y_pred, average='weighted')\n",
    "# f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "# print(f'Precision: {precision:.2f}')\n",
    "# print(f'Recall: {recall:.2f}')\n",
    "# print(f'F1 Score: {f1:.2f}')\n",
    "# print('\\nClassification Report:\\n', classification_report(y_test, y_pred))\n",
    "# ```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
